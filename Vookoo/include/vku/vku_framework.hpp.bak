////////////////////////////////////////////////////////////////////////////////
//
// Demo framework for the Vookoo for the Vookoo high level C++ Vulkan interface.
//
// (C) Andy Thomason 2017 MIT License
//
// This is an optional demo framework for the Vookoo high level C++ Vulkan interface.
//
////////////////////////////////////////////////////////////////////////////////

#ifndef VKU_FRAMEWORK_HPP
#define VKU_FRAMEWORK_HPP

#ifdef _WIN32
#ifndef WIN32_LEAN_AND_MEAN
#define WIN32_LEAN_AND_MEAN
#endif
#ifndef VC_EXTRALEAN
#define VC_EXTRALEAN
#endif

#ifndef VK_USE_PLATFORM_WIN32_KHR
#define VK_USE_PLATFORM_WIN32_KHR
#endif
#define GLFW_EXPOSE_NATIVE_WIN32
#define VKU_SURFACE "VK_KHR_win32_surface"
#pragma warning(disable : 4005)
#else
#define VK_USE_PLATFORM_XLIB_KHR
#define GLFW_EXPOSE_NATIVE_X11
#define VKU_SURFACE "VK_KHR_xlib_surface"
#endif
#define VK_MAJOR_VERSION 1
#define VK_MINOR_VERSION 1

#include <vku/vku.hpp>		// must place here

#ifndef VKU_NO_GLFW
#define GLFW_EXPOSE_NATIVE_WIN32
//#define GLFW_INCLUDE_VULKAN // not required here, as vulkan.h is already included
#include <GLFW/glfw3.h>
#include <GLFW/glfw3native.h>
#endif

// Undo damage done by windows.h
#undef APIENTRY
#undef None
#undef max
#undef min

#include <array>
#include <fstream>
#include <iostream>
#include <unordered_map>
#include <vector>
#include <thread>
#include <chrono>
#include <functional>
#include <cstddef>
#include <set>



namespace vku {

	static inline constexpr vk::SampleCountFlagBits const DefaultSampleCount(vk::SampleCountFlagBits::e4); // 4xMSAA guarenteed supported, higher not really needed and can be overriden in driver control panel (radeon/nvidia tweaking) by user if wanted to enhance at a great loss in performance

/// This class provides an optional interface to the vulkan instance, devices and queues.
/// It is not used by any of the other classes directly and so can be safely ignored if Vookoo
/// is embedded in an engine.
/// See https://vulkan-tutorial.com for details of many operations here.
class Framework {
public:
  Framework() {
  }

  // Construct a framework containing the instance, a device and one or more queues.
  void FrameworkCreate(const std::string &name) {
	  uint32_t  const apiVersion(VK_MAKE_VERSION(VK_MAJOR_VERSION, VK_MINOR_VERSION, 0));
    vku::InstanceMaker im{};
    im.defaultLayers();
	im.applicationName(name.c_str());
	im.engineName("supersinfulsilicon");
	im.applicationVersion(1);
	im.engineVersion(1);
	im.apiVersion(apiVersion);

    instance_ = im.createUnique();

#ifndef NDEBUG
    callback_ = DebugCallback(*instance_);
#endif

    auto const pds = instance_->enumeratePhysicalDevices();
	for (auto const& i : pds)
	{
		uint32_t const physicalDeviceApiVersion = i.getProperties().apiVersion;
		if (physicalDeviceApiVersion >= apiVersion) {
			physical_device_ = i;
			fmt::print(fg(fmt::color::magenta),  "[ Vulkan {:d}.{:d} ]" "\n", VK_VERSION_MAJOR(physicalDeviceApiVersion), 
																		      VK_VERSION_MINOR(physicalDeviceApiVersion));
			fmt::print(fg(fmt::color::white),   "[ {:s} ]" "\n", i.getProperties().deviceName);
			break;
		}
	}
	if (nullptr == physical_device_) {
		fmt::print(fg(fmt::color::red),   "[ ! Vulkan 1.1 - Not supported by any gpu device ! ]" "\n");
		return;
	}
    auto qprops = physical_device_.getQueueFamilyProperties();
    
    graphicsQueueFamilyIndex_ = 0;
    computeQueueFamilyIndex_ = 0;
	transferQueueFamilyIndex_ = 0;
	vk::QueueFlags  
		searchGraphics = vk::QueueFlagBits::eGraphics,
		searchCompute = vk::QueueFlagBits::eCompute,  // **************** any ShaderReadOnlyOptimal (not general)sampler access in compute shader requires graphics queue aswell. Found out also that for compute to not stall the raphics pieline, it must be on a seperate queue, then it is correctly async compute
		searchTransfer = vk::QueueFlagBits::eTransfer; // speedy 8x8 granularity (multiple of 8) transfer queue
																		// ** resolution must be divisible by 8 (all normally are)
    // Look for an omnipurpose queue family first
    // It is better if we can schedule operations without barriers and semaphores.
    // The Spec says: "If an implementation exposes any queue family that supports graphics operations,
    // at least one queue family of at least one physical device exposed by the implementation
    // must support both graphics and compute operations."
    // Also: All commands that are allowed on a queue that supports transfer operations are
    // also allowed on a queue that supports either graphics or compute operations...
    // As a result we can expect a queue family with at least all three and maybe all four modes.

	std::set<uint32_t> enabledQueueIndices;
			
    for (int32_t qi = (int32_t)qprops.size() - 1; qi >= 0; --qi) {	// start from back to capture unique queues first
      auto &qprop = qprops[qi];

      if (searchGraphics && (qprop.queueFlags & searchGraphics) == searchGraphics) {
			graphicsQueueFamilyIndex_ = qi;
			enabledQueueIndices.emplace(qi);
			if (0 == qi) {
				searchGraphics = (vk::QueueFlagBits)0; // prevent further search only if equal to zero for graphics queue (default index)
			}
			FMT_LOG_OK(GPU_LOG, "Graphics Queue Selected < {:s} >", vk::to_string(qprop.queueFlags));
      }
	  if (searchCompute && (qprop.queueFlags & searchCompute) == searchCompute) {
		  computeQueueFamilyIndex_ = qi;
		  enabledQueueIndices.emplace(qi);
		  searchCompute = (vk::QueueFlagBits)0; // prevent further search
		  FMT_LOG_OK(GPU_LOG, "Compute Queue Selected < {:s} >", vk::to_string(qprop.queueFlags)); 
	  }
	  if (searchTransfer && (qprop.queueFlags & searchTransfer) == searchTransfer) {
		  transferQueueFamilyIndex_ = qi;
		  enabledQueueIndices.emplace(qi);
		  searchTransfer = (vk::QueueFlagBits)0;
		  FMT_LOG(GPU_LOG, "Present Queue Pending < {:s} >\n", vk::to_string(qprop.queueFlags));
	  }
    }

    memprops_ = physical_device_.getMemoryProperties();

	VkPhysicalDeviceVulkanMemoryModelFeaturesKHR supportedMemoryModel{ VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_MEMORY_MODEL_FEATURES_KHR , nullptr };
	vk::PhysicalDeviceFeatures2 supportedFeatures{};
	supportedFeatures.pNext = &supportedMemoryModel;
	physical_device_.getFeatures2(&supportedFeatures);

	vk::PhysicalDeviceFeatures enabledFeatures{};
	enabledFeatures.geometryShader = supportedFeatures.features.geometryShader;
	enabledFeatures.samplerAnisotropy = supportedFeatures.features.samplerAnisotropy;
	//enabledFeatures.independentBlend = supportedFeatures.independentBlend; // not required
	//enabledFeatures.robustBufferAccess = supportedFeatures.robustBufferAccess; // safer but a lot slower good for debugging out of bounds access
	enabledFeatures.textureCompressionBC = supportedFeatures.features.textureCompressionBC;
	enabledFeatures.shaderStorageImageExtendedFormats = supportedFeatures.features.shaderStorageImageExtendedFormats;
	enabledFeatures.vertexPipelineStoresAndAtomics = supportedFeatures.features.vertexPipelineStoresAndAtomics;
	enabledFeatures.fragmentStoresAndAtomics = supportedFeatures.features.fragmentStoresAndAtomics;

	PRINT_FEATURE(supportedMemoryModel.vulkanMemoryModel, "vulkan memory model"); if (!supportedMemoryModel.vulkanMemoryModel) return;
	PRINT_FEATURE(enabledFeatures.geometryShader, "geometry shader"); if (!enabledFeatures.geometryShader) return;
	PRINT_FEATURE(enabledFeatures.samplerAnisotropy, "anisotropic filtering"); if (!enabledFeatures.samplerAnisotropy) return;
	PRINT_FEATURE(enabledFeatures.textureCompressionBC, "texture compression"); if (!enabledFeatures.textureCompressionBC) return;
	PRINT_FEATURE(enabledFeatures.shaderStorageImageExtendedFormats, "extended compute image formats"); if (!enabledFeatures.shaderStorageImageExtendedFormats) return;
	PRINT_FEATURE(enabledFeatures.vertexPipelineStoresAndAtomics, "vertex image ops"); if (!enabledFeatures.vertexPipelineStoresAndAtomics) return;	  // use of image operations in vertex shader requires this feature to be enabled
	PRINT_FEATURE(enabledFeatures.fragmentStoresAndAtomics, "fragment image ops"); if (!enabledFeatures.fragmentStoresAndAtomics) return;	  // use of image operations in vertex shader requires this feature to be enabled

    vku::DeviceMaker dm{};
    dm.defaultLayers();

	// add extensions
	bool supported(false);
	auto const extensions = physical_device_.enumerateDeviceExtensionProperties();

	// Required Extensions //
	ADD_EXTENSION(extensions, dm, VK_KHR_VULKAN_MEMORY_MODEL_EXTENSION_NAME, supported); if (!supported) return;
	ADD_EXTENSION(extensions, dm, VK_KHR_GET_MEMORY_REQUIREMENTS_2_EXTENSION_NAME, supported); if (!supported) return;
	ADD_EXTENSION(extensions, dm, VK_KHR_DEDICATED_ALLOCATION_EXTENSION_NAME, supported); if (!supported) return;
	ADD_EXTENSION(extensions, dm, VK_EXT_GLOBAL_PRIORITY_EXTENSION_NAME, supported); if (!supported) return;
	ADD_EXTENSION(extensions, dm, VK_KHR_CREATE_RENDERPASS_2_EXTENSION_NAME, supported); if (!supported) return;
	ADD_EXTENSION(extensions, dm, VK_KHR_DEPTH_STENCIL_RESOLVE_EXTENSION_NAME, supported); if (!supported) return;

	// Optional/Additional Extensions //
#if defined(FULLSCREEN_EXCLUSIVE) && defined(VK_EXT_full_screen_exclusive)
	
	ADD_EXTENSION(extensions, dm, VK_EXT_FULL_SCREEN_EXCLUSIVE_EXTENSION_NAME, supported);
	bFullScreenExclusiveExtensionSupported = supported;
	
#endif

	// Extension Configuration //
	vk::PhysicalDeviceDepthStencilResolvePropertiesKHR depthResolveProperties{};
	vk::PhysicalDeviceProperties2 physical_properties{};
	physical_properties.pNext = &depthResolveProperties;
	physical_device_.getProperties2(&physical_properties);

	if (vk::ResolveModeFlagBitsKHR::eMin & depthResolveProperties.supportedDepthResolveModes) {
		depthResolveMode_ = vk::ResolveModeFlagBitsKHR::eMin;
	}
	else {
		depthResolveMode_ = vk::ResolveModeFlagBitsKHR::eSampleZero;
	}
	
	// use same mode for stencil if mode is available or...
	if (depthResolveMode_ & depthResolveProperties.supportedStencilResolveModes) {
		stencilResolveMode_ = depthResolveMode_;
	}
	else {
		stencilResolveMode_ = vk::ResolveModeFlagBitsKHR::eSampleZero;
	}

	// ... must be equal if independent resolve is not supported
	// see https://www.khronos.org/registry/vulkan/specs/1.1-extensions/man/html/VkSubpassDescriptionDepthStencilResolveKHR.html
	if (!depthResolveProperties.independentResolve && !depthResolveProperties.independentResolveNone) {
		stencilResolveMode_ = depthResolveMode_;
	}
	// validatw
	if (!(depthResolveMode_ & depthResolveProperties.supportedDepthResolveModes) || !(stencilResolveMode_ & depthResolveProperties.supportedStencilResolveModes)) {
		FMT_LOG_FAIL(GPU_LOG, "Unsupported DepthStencil resolve mode\n");
	}
	else {
		FMT_LOG_OK(GPU_LOG, "DepthStencil resolve mode ({:s}), ({:s})", vk::to_string(depthResolveMode_), vk::to_string(stencilResolveMode_));
	}

    dm.queue<VK_QUEUE_GLOBAL_PRIORITY_REALTIME_EXT>(graphicsQueueFamilyIndex_); // Add graphics queue as first queue for device
	enabledQueueIndices.erase(graphicsQueueFamilyIndex_); // remove from rest of enabled queue indices
	// pre-liminary enable extra queues (compute and transfer/present)
	for (auto const queueIndexEnabled : enabledQueueIndices) {
		dm.queue<VK_QUEUE_GLOBAL_PRIORITY_REALTIME_EXT>(queueIndexEnabled);
	}	

	// start of pNext linked list chain for device creation
	
	VkPhysicalDeviceVulkanMemoryModelFeaturesKHR const memoryModel{
		VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_MEMORY_MODEL_FEATURES_KHR,
		nullptr, // pNext Chain /##################### ########/
		VK_TRUE,	// mandatory support for base Vulkan Memory Model
		supportedMemoryModel.vulkanMemoryModelDeviceScope,
		supportedMemoryModel.vulkanMemoryModelAvailabilityVisibilityChains
	};
    device_ = dm.createUnique(physical_device_, enabledFeatures, &memoryModel);
    
    vk::PipelineCacheCreateInfo pipelineCacheInfo{};
    pipelineCache_ = device_->createPipelineCacheUnique(pipelineCacheInfo);

    std::vector<vk::DescriptorPoolSize> poolSizes;
    poolSizes.emplace_back(vk::DescriptorType::eUniformBuffer, MAX_NUM_UNIFORM_BUFFERS);
    poolSizes.emplace_back(vk::DescriptorType::eCombinedImageSampler, MAX_NUM_IMAGES);
    poolSizes.emplace_back(vk::DescriptorType::eStorageBuffer, MAX_NUM_STORAGE_BUFFERS);

    // Create an arbitrary number of descriptors in a pool.
    // Allow the descriptors to be freed, possibly not optimal behaviour.
    vk::DescriptorPoolCreateInfo descriptorPoolInfo{};
	//descriptorPoolInfo.flags = // vk::DescriptorPoolCreateFlagBits::eFreeDescriptorSet;
    descriptorPoolInfo.maxSets = MAX_NUM_DESCRIPTOR_SETS;
    descriptorPoolInfo.poolSizeCount = (uint32_t)poolSizes.size();
    descriptorPoolInfo.pPoolSizes = poolSizes.data();
    descriptorPool_ = device_->createDescriptorPoolUnique(descriptorPoolInfo);

#ifndef NDEBUG
	callback_.acquireDeviceFunctionPointers(*device_);
#endif

    ok_ = true;
  }

  void dumpCaps(std::ostream &os) const {
    os << "Memory Types\n";
    for (uint32_t i = 0; i != memprops_.memoryTypeCount; ++i) {
      os << "  type" << i << " heap" << memprops_.memoryTypes[i].heapIndex << " " << vk::to_string(memprops_.memoryTypes[i].propertyFlags) << "\n";
    }
    os << "Heaps\n";
    for (uint32_t i = 0; i != memprops_.memoryHeapCount; ++i) {
      os << "  heap" << vk::to_string(memprops_.memoryHeaps[i].flags) << " " << memprops_.memoryHeaps[i].size << "\n";
    }
  }

  /// Get the Vulkan instance.
  const vk::Instance instance() const { return *instance_; }

  /// Get the Vulkan device.
  const vk::Device device() const { return *device_; }

  /// Get the queue used to submit graphics jobs
  const vk::Queue graphicsQueue() const { return device_->getQueue(graphicsQueueFamilyIndex_, 0); }

  /// Get the queue used to submit compute jobs
  const vk::Queue computeQueue() const { return device_->getQueue(computeQueueFamilyIndex_, 0); }

  // Get the queue used to transfer data
  const vk::Queue transferQueue() const { return device_->getQueue(transferQueueFamilyIndex_, 0); }

  /// Get the physical device.
  const vk::PhysicalDevice &physicalDevice() const { return physical_device_; }

  /// Get the default pipeline cache (you can use your own if you like).
  const vk::PipelineCache pipelineCache() const { return *pipelineCache_; }

  /// Get the default descriptor pool (you can use your own if you like).
  const vk::DescriptorPool descriptorPool() const { return *descriptorPool_; }

  /// Get the family index for the graphics queues.
  uint32_t graphicsQueueFamilyIndex() const { return graphicsQueueFamilyIndex_; }

  /// Get the family index for the compute queues.
  uint32_t computeQueueFamilyIndex() const { return computeQueueFamilyIndex_; }

  /// Get the family index for the compute queues.
  uint32_t transferQueueFamilyIndex() const { return transferQueueFamilyIndex_; }

  const vk::PhysicalDeviceMemoryProperties &memprops() const { return memprops_; }

  /// Clean up the framework satisfying the Vulkan verification layers.
  ~Framework() {
    if (device_) {
      device_->waitIdle();
      if (pipelineCache_) {
        pipelineCache_.reset();
      }
      if (descriptorPool_) {
        descriptorPool_.reset();
      }
      device_.reset();
    }

    if (instance_) {
#ifndef NDEBUG
      callback_.reset();
#endif
      instance_.reset();
    }
  }

  Framework &operator=(Framework &&rhs) = default;

  // extensions supported ? //
  vk::ResolveModeFlagBitsKHR const getDepthResolveMode() const {
	  return(depthResolveMode_);
  }
  vk::ResolveModeFlagBitsKHR const getStencilResolveMode() const {
	  return(stencilResolveMode_);
  }

  void setFullScreenExclusiveEnabled(bool const bEnabled) {
	  bFullScreenExclusiveExtensionEnabled = bEnabled;
  }
  bool const isFullScreenExclusiveExtensionSupported() const {
	  return(bFullScreenExclusiveExtensionEnabled & bFullScreenExclusiveExtensionSupported);
  }

  /// Returns true if the Framework has been built correctly.
  bool ok() const { return ok_; }

private:
  vk::UniqueInstance instance_;
#ifndef NDEBUG
  vku::DebugCallback callback_;
#endif
  vk::UniqueDevice device_;
  vk::PhysicalDevice physical_device_;
  vk::UniquePipelineCache pipelineCache_;
  vk::UniqueDescriptorPool descriptorPool_;
  uint32_t graphicsQueueFamilyIndex_;
  uint32_t computeQueueFamilyIndex_;
  uint32_t transferQueueFamilyIndex_;
  vk::PhysicalDeviceMemoryProperties memprops_;

  vk::ResolveModeFlagBitsKHR depthResolveMode_{}, stencilResolveMode_{};
	
  // extensions supported ? //
  bool bFullScreenExclusiveExtensionEnabled = false,
	   bFullScreenExclusiveExtensionSupported = false;

  bool ok_ = false;
};

/// This class wraps a window, a surface and a swap chain for that surface.
BETTER_ENUM(eCommandPools, uint32_t const, DEFAULT_POOL = 0, OVERLAY_POOL, SCATTERED_POOL, TRANSIENT_POOL, DMA_TRANSFER_POOL_THREAD_PRIMARY, DMA_TRANSFER_POOL_THREAD_SECONDARY, COMPUTE_POOL);
BETTER_ENUM(eGraphicsEvent, uint32_t const, RESERVED_PLACEHOLDER = 0, RENDERED_STATIC);
BETTER_ENUM(eFrameBuffers, uint32_t const, COLOR_DEPTH, HALF_COLOR_ONLY, MID_COLOR_ONLY, COLOR_DEPTHREAD, PRESENT);
BETTER_ENUM(eOverlayBuffers, uint32_t const, TRANSFER, RENDER);
BETTER_ENUM(eComputeBuffers, uint32_t const, TRANSFER_LIGHT, COMPUTE_LIGHT);

class Window {

	Framework const& fw_;	// reference to framework!

public:
	Window(vku::Framework const& fw ) : fw_(fw) {}

#ifndef VKU_NO_GLFW
  /// Construct a window, surface and swapchain using a GLFW window.
  Window(vku::Framework const & fw, const vk::Device &device, const vk::PhysicalDevice &physicalDevice, uint32_t const graphicsQueueFamilyIndex, uint32_t const computeQueueFamilyIndex, uint32_t const transferQueueFamilyIndex, GLFWwindow * const window, bool const bVsyncDisabled)
	  : fw_(fw)
  {
#ifdef VK_USE_PLATFORM_WIN32_KHR
    auto module = GetModuleHandle(nullptr);
    auto const handle = glfwGetWin32Window(window);
	glfwSetWindowUserPointer(window, this);
    auto ci = vk::Win32SurfaceCreateInfoKHR{{}, module, handle};
    auto const surface = fw.instance().createWin32SurfaceKHR(ci);
	auto const monitor = MonitorFromWindow(handle, MONITOR_DEFAULTTOPRIMARY);
#endif
#ifdef VK_USE_PLATFORM_XLIB_KHR
    auto display = glfwGetX11Display();
    auto x11window = glfwGetX11Window(window);
    auto ci = vk::XlibSurfaceCreateInfoKHR{{}, display, x11window};
    auto surface = instance.createXlibSurfaceKHR(ci);
#endif
    init(fw.instance(), device, physicalDevice, graphicsQueueFamilyIndex, computeQueueFamilyIndex, transferQueueFamilyIndex, surface, monitor, bVsyncDisabled);
  }
#endif

 /* void downsampleDepth(vk::CommandBuffer const&__restrict cb)
  {
	  // layout transitions must be set appropriately b4 function

	  vk::ImageSubresourceLayers const srcLayer(vk::ImageAspectFlagBits::eDepth, 0, 0, 1);
	  vk::ImageSubresourceLayers const dstLayer(vk::ImageAspectFlagBits::eDepth, 0, 0, 1);

	  std::array<vk::Offset3D, 2> const srcOffsets = { vk::Offset3D(), vk::Offset3D(width_, height_, 1) };
	  std::array<vk::Offset3D, 2> const dstOffsets = { vk::Offset3D(), vk::Offset3D(width_ / vku::DOWN_RES_FACTOR, height_ / vku::DOWN_RES_FACTOR, 1) };

	  vk::ImageBlit const region(srcLayer, srcOffsets, dstLayer, dstOffsets);

	  cb.blitImage(depthImage_.image(), vk::ImageLayout::eTransferSrcOptimal, depthImageDown_.image(), vk::ImageLayout::eTransferDstOptimal, 1, &region, vk::Filter::eNearest);
  }
  */
  /*
  void copyLastRenderedImage(vk::CommandBuffer const& __restrict cb, vku::TextureImage2D* const __restrict dstImage, uint32_t const last_image_index)
  {
	  // layout transitions must be set appropriately b4 function for destination image
	  // swap chain image is managed inside this function only
	  vk::ImageMemoryBarrier imageMemoryBarriers = {};
	  imageMemoryBarriers.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
	  imageMemoryBarriers.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
	  imageMemoryBarriers.image = images_[last_image_index];
	  imageMemoryBarriers.subresourceRange = { vk::ImageAspectFlagBits::eColor, 0, 1, 0, 1 };

	  vk::PipelineStageFlags srcStageMask{};
	  vk::PipelineStageFlags dstStageMask{};
	  vk::DependencyFlags dependencyFlags{};
	  vk::AccessFlags srcMask{};
	  vk::AccessFlags dstMask{};

	  typedef vk::ImageLayout il;
	  typedef vk::PipelineStageFlagBits psfb;
	  typedef vk::AccessFlagBits afb;

	  imageMemoryBarriers.oldLayout = vk::ImageLayout::ePresentSrcKHR;
	  imageMemoryBarriers.newLayout = vk::ImageLayout::eTransferSrcOptimal;
	  srcMask = afb::eMemoryRead; srcStageMask = psfb::eBottomOfPipe;
	  dstMask = afb::eTransferRead; dstStageMask = psfb::eTransfer;

	  imageMemoryBarriers.srcAccessMask = srcMask;
	  imageMemoryBarriers.dstAccessMask = dstMask;
	  auto memoryBarriers = nullptr;
	  auto bufferMemoryBarriers = nullptr;
	  cb.pipelineBarrier(srcStageMask, dstStageMask, dependencyFlags, memoryBarriers, bufferMemoryBarriers, imageMemoryBarriers);


	  // do gpu -> gpu local blit
	  // must be a blit because we need it back in linear color space, swapchain images are srgb textures
	  // the blit performs the conversion back to linear!
	  {
		  vk::ImageSubresourceLayers const srcLayer(vk::ImageAspectFlagBits::eColor, 0, 0, 1);
		  vk::ImageSubresourceLayers const dstLayer(vk::ImageAspectFlagBits::eColor, 0, 0, 1);

		  std::array<vk::Offset3D, 2> const srcOffsets = { vk::Offset3D(), vk::Offset3D(width_, height_, 1) };
		  std::array<vk::Offset3D, 2> const dstOffsets = { vk::Offset3D(), vk::Offset3D(width_, height_, 1) };

		  vk::ImageBlit const region(srcLayer, srcOffsets, dstLayer, dstOffsets);

		  cb.blitImage(images_[last_image_index], vk::ImageLayout::eTransferSrcOptimal, dstImage->image(), vk::ImageLayout::eTransferDstOptimal, 1, &region, vk::Filter::eNearest);
	  }


	  // automatically transition last rendered swapchain image back to swapchain friendly layout
	  imageMemoryBarriers.oldLayout = vk::ImageLayout::eTransferSrcOptimal;
	  imageMemoryBarriers.newLayout = vk::ImageLayout::ePresentSrcKHR;
	  srcMask = afb::eTransferRead; srcStageMask = psfb::eTransfer;
	  dstMask = afb::eMemoryRead; dstStageMask = psfb::eBottomOfPipe;

	  imageMemoryBarriers.srcAccessMask = srcMask;
	  imageMemoryBarriers.dstAccessMask = dstMask;
	  cb.pipelineBarrier(srcStageMask, dstStageMask, dependencyFlags, memoryBarriers, bufferMemoryBarriers, imageMemoryBarriers);
  }*/

  private:

  bool const recreateSwapChain()
  {
	  fmt::print(fg(fmt::color::lime_green), "creating swapchain.... " "\n");

	  vk::SurfaceCapabilities2KHR surfaceCaps;
	  vk::PhysicalDeviceSurfaceInfo2KHR surfaceInfo(surface_);

#if defined(FULLSCREEN_EXCLUSIVE) && defined(VK_EXT_full_screen_exclusive)
	  vk::SurfaceFullScreenExclusiveWin32InfoEXT full_screen_exclusive_win32(monitor_);
	  vk::SurfaceFullScreenExclusiveInfoEXT full_screen_exclusive{ vk::FullScreenExclusiveEXT::eApplicationControlled };
	  vk::SurfaceCapabilitiesFullScreenExclusiveEXT surfaceCapFullscreenExclusive{};

	  if (nullptr == monitor_) {
		  fmt::print(fg(fmt::color::orange), "fullscreen exclusive disabled." "\n");
	  }
	  else {
		  
		  if (fw_.isFullScreenExclusiveExtensionSupported()) {
			  full_screen_exclusive.pNext = &full_screen_exclusive_win32;
			  surfaceInfo.pNext = &full_screen_exclusive;
			  surfaceCaps.pNext = &surfaceCapFullscreenExclusive;
		  }
	  }
#endif

	  physicalDevice_.getSurfaceCapabilities2KHR(&surfaceInfo, &surfaceCaps);
	  width_ = surfaceCaps.surfaceCapabilities.currentExtent.width;
	  height_ = surfaceCaps.surfaceCapabilities.currentExtent.height;

#if defined(FULLSCREEN_EXCLUSIVE) && defined(VK_EXT_full_screen_exclusive)
	  if (monitor_ && fw_.isFullScreenExclusiveExtensionSupported()) {
		  bFullScreenExclusiveSupported = surfaceCapFullscreenExclusive.fullScreenExclusiveSupported;
		  if (bFullScreenExclusiveSupported) {
			  fmt::print(fg(fmt::color::lime_green), "fullscreen exclusive supported\n");
		  }
		  else {
			  fmt::print(fg(fmt::color::orange), "fullscreen exclusive not supported\n"); // allow loading to continue, will still work just not optimal
		  }
	  }
	  else { // Extension doesn't exist or user settings ini has disabled exclusivity, silently fail/disable exclusive fullscreen
		  bFullScreenExclusiveSupported = false;
	  }
#endif
	  auto fmts = physicalDevice_.getSurfaceFormats2KHR(surfaceInfo);
	  swapchainImageFormat_ = fmts[0].surfaceFormat.format;
	  swapchainColorSpace_ = fmts[0].surfaceFormat.colorSpace;
	  if (fmts.size() == 1 && swapchainImageFormat_ == vk::Format::eUndefined) {
		  swapchainImageFormat_ = vk::Format::eB8G8R8A8Unorm;
		  swapchainColorSpace_ = vk::ColorSpaceKHR::eSrgbNonlinear;
	  }
	  else {
		  for (auto& fmt : fmts) {
			  if (fmt.surfaceFormat.format == vk::Format::eB8G8R8A8Unorm && fmt.surfaceFormat.colorSpace == vk::ColorSpaceKHR::eSrgbNonlinear) {
				  swapchainImageFormat_ = fmt.surfaceFormat.format;
				  swapchainColorSpace_ = fmt.surfaceFormat.colorSpace;
				  break;
			  }
		  }
	  }
	  if (swapchainImageFormat_ == vk::Format::eB8G8R8A8Unorm && swapchainColorSpace_ == vk::ColorSpaceKHR::eSrgbNonlinear) {
		  fmt::print(fg(fmt::color::lime_green), "32bit SRGB Backbuffer");
	  }
	  else {
		  fmt::print(fg(fmt::color::red), "[FAIL] 32bit NON-SRGB Backbuffer");
		  return(false); // this is critical, would make everything extremely washed out or extremely dark, fail launch completely so game never pubicly looks like this
	  }

	  auto pms = physicalDevice_.getSurfacePresentModes2EXT(surfaceInfo);
	  vk::PresentModeKHR swapchainPresentMode = pms[0]; // default to first available

	  if (vsyncDisabled_)
	  {
		  if (std::find(pms.begin(), pms.end(), vk::PresentModeKHR::eImmediate) != pms.end()) {
			  swapchainPresentMode = vk::PresentModeKHR::eImmediate;
			  fmt::print(fg(fmt::color::lime_green), " - (VSYNC forced off) - ");
		  }
		  else {
			  fmt::print(fg(fmt::color::lime_green), " - (VSYNC could not be forced off) - ");
		  }
	  }
	  else {
		  // in order of preference
		  if (std::find(pms.begin(), pms.end(), vk::PresentModeKHR::eMailbox) != pms.end()) {
			  swapchainPresentMode = vk::PresentModeKHR::eMailbox;
		  }
		  else if (std::find(pms.begin(), pms.end(), vk::PresentModeKHR::eFifoRelaxed) != pms.end()) {
			  swapchainPresentMode = vk::PresentModeKHR::eFifoRelaxed;
		  }
		  else if (std::find(pms.begin(), pms.end(), vk::PresentModeKHR::eFifo) != pms.end()) {
			  swapchainPresentMode = vk::PresentModeKHR::eFifo;
		  }
	  }
	  fmt::print(fg(fmt::color::lime_green), " < {:s} >" "\n", vk::to_string(swapchainPresentMode));

	  vk::SwapchainCreateInfoKHR swapinfo{};
	  std::array<uint32_t, 2> const queueFamilyIndices = { graphicsQueueFamily_, presentQueueFamily_ };
	  bool const sameQueues = (queueFamilyIndices[0] == queueFamilyIndices[1]);

	  swapinfo.surface = surface_;
	  swapinfo.minImageCount = surfaceCaps.surfaceCapabilities.minImageCount + 1U + TRIPLE_BUFFERED;		// min(1) + 1(double buffered) + 1(triple buffered)
	  swapinfo.imageFormat = swapchainImageFormat_;
	  swapinfo.imageColorSpace = swapchainColorSpace_;
	  swapinfo.imageExtent = surfaceCaps.surfaceCapabilities.currentExtent;
	  swapinfo.imageArrayLayers = 1;
	  swapinfo.imageUsage = vk::ImageUsageFlagBits::eColorAttachment;
	  swapinfo.imageSharingMode = vk::SharingMode::eExclusive;	// best to use Exclusive Sharing Mode for performance optimizaion: https://gpuopen.com/vulkan-and-doom/ 
	  swapinfo.queueFamilyIndexCount = !sameQueues ? 2 : 0;
	  swapinfo.pQueueFamilyIndices = queueFamilyIndices.data();
	  swapinfo.preTransform = surfaceCaps.surfaceCapabilities.currentTransform;
	  swapinfo.compositeAlpha = vk::CompositeAlphaFlagBitsKHR::eOpaque;
	  swapinfo.presentMode = swapchainPresentMode;
	  swapinfo.clipped = VK_TRUE;
	  swapinfo.oldSwapchain = (swapchain_ ? *swapchain_ : vk::SwapchainKHR{});

#if defined(FULLSCREEN_EXCLUSIVE) && defined(VK_EXT_full_screen_exclusive)

	  if (bFullScreenExclusiveSupported) {

		  swapinfo.pNext = &full_screen_exclusive;

	  }

#endif

	  // release old image views if they exists
	  for (auto& iv : imageViews_) {
		  device_.destroyImageView(iv);
	  }
	  images_.clear();

	  vk::UniqueSwapchainKHR swapchain = device_.createSwapchainKHRUnique(swapinfo);
	  if (swapchain_) {
		  device_.destroySwapchainKHR(*swapchain_);  
	  }
	  swapchain_.swap(swapchain);

	  images_ = device_.getSwapchainImagesKHR(*swapchain_);
	  for (auto& img : images_) {
		  vk::ImageViewCreateInfo ci{};
		  ci.image = img;
		  ci.viewType = vk::ImageViewType::e2D;
		  ci.format = swapchainImageFormat_;
		  ci.subresourceRange = { vk::ImageAspectFlagBits::eColor, 0, 1, 0, 1 };
		  imageViews_.emplace_back(device_.createImageView(ci));
	  }

	  return(true);
  }

  void preAcquireFirstImage()
  {
	  // acquire first image ahead of time to prime the render present queue in the order we expect
	  uint32_t realImageIndex(0);
	  vk::FenceCreateInfo fci{};
	  vk::UniqueFence vkTmp(device_.createFence(fci));

	  device_.acquireNextImageKHR(*swapchain_, std::numeric_limits<uint64_t>::max(), vk::Semaphore(), *vkTmp, &realImageIndex);

	  vkTmp.release();
  }

  public:

  void init(const vk::Instance &instance, const vk::Device &device, const vk::PhysicalDevice &physicalDevice, uint32_t const graphicsQueueFamilyIndex, uint32_t const computeQueueFamilyIndex, uint32_t const transferQueueFamilyIndex, vk::SurfaceKHR const surface, HMONITOR const& monitor, bool const bVsyncDisabled) {
	  
	  surface_ = surface;
	  instance_ = instance;
	  device_ = device;
	  
	  // neccessary to cache for hot swapchain recreation
	  physicalDevice_ = physicalDevice;
	  graphicsQueueFamily_ = graphicsQueueFamilyIndex;
	  monitor_ = monitor;
	  vsyncDisabled_ = bVsyncDisabled;

	  presentQueueFamily_ = 0;
	  auto const qprops = physicalDevice_.getQueueFamilyProperties();

	  // start from back to capture unique present transfer optimized queue (DMA)
	  // otherwise default to queue 0 (default) if surface is unsupported
	  for (int32_t qi = (int32_t)qprops.size() - 1; qi >= 0 ; --qi) {
		  auto const& qprop = qprops[qi];											// todo: bug here with flickering geometry (release mode only) if graphics queue is used for present instead....
		  if (physicalDevice_.getSurfaceSupportKHR(qi, surface_) && (qprop.queueFlags & vk::QueueFlagBits::eTransfer) == vk::QueueFlagBits::eTransfer) {
			  
			  presentQueueFamily_ = qi;
			  FMT_LOG_OK(GPU_LOG, "Present Queue Selected < {:s} >", vk::to_string(qprop.queueFlags));
			  break;
		  }
	  }
	  
	  // initial creation of swapchain
	  if (!recreateSwapChain()) {
		  FMT_LOG_FAIL(GPU_LOG, "Major swapchain fail\n");
		  return;
	  }

	  {
		  vk::CommandPoolCreateInfo cpci{ vk::CommandPoolCreateFlagBits::eTransient, graphicsQueueFamilyIndex };
		  commandPool_[eCommandPools::TRANSIENT_POOL] = device.createCommandPoolUnique(cpci);
	  }

	  auto memprops = physicalDevice.getMemoryProperties();

	  colorImage_ = vku::ColorAttachmentImage(device, memprops, width_, height_, vku::DefaultSampleCount, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0), false, false, swapchainImageFormat_);	// is not sampled, is not inputattachment
	  depthImage_ = vku::DepthAttachmentImage(device, memprops, width_, height_, vku::DefaultSampleCount, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0), false, true);  // is inputattachment

	  lastColorImage_ = vku::ColorAttachmentImage(device, memprops, width_, height_, vk::SampleCountFlagBits::e1, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0), true, false, swapchainImageFormat_);	// is sampled, not inputattachment
	  
	  colorVolumetricImageDown_ = vku::ColorAttachmentImage(device, memprops, width_ / vku::DOWN_RES_FACTOR, height_ / vku::DOWN_RES_FACTOR, vk::SampleCountFlagBits::e1, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0), true, false, swapchainImageFormat_);  // is sampled, not inputattachment
	 
	  depthImageResolve_[0] = vku::DepthImage(device, memprops, width_, height_, 1U, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0));  // D32 depth only image
	  depthImageResolve_[1] = vku::DepthImage(device, memprops, width_ / vku::DOWN_RES_FACTOR, height_ / vku::DOWN_RES_FACTOR, 1U, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0));  // D32 depth only image

	  guiImage_[0] = vku::ColorAttachmentImage(device, memprops, width_, height_, vku::DefaultSampleCount, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0), false, false, swapchainImageFormat_);	// not sampled, is not inputattachment
	  guiImage_[1] = vku::ColorAttachmentImage(device, memprops, width_, height_, vk::SampleCountFlagBits::e1, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0), false, true, swapchainImageFormat_);	// not sampled, is inputattachment

	  colorVolumetricImageDown_ = vku::ColorAttachmentImage(device, memprops, width_ / vku::DOWN_RES_FACTOR, height_ / vku::DOWN_RES_FACTOR, vk::SampleCountFlagBits::e1, *commandPool_[eCommandPools::TRANSIENT_POOL], device.getQueue(graphicsQueueFamilyIndex, 0), true, false, swapchainImageFormat_);  // is sampled, not inputattachment

	  // Build the renderpass using two attachments, colour and depth/stencil. (regular rendering pass)
	  {
		  vku::RenderpassMaker rpm;

		  // **** SUBPASS - Regular rendering ZONLY No Color Writes//
		 
		  // The depth/stencil attachment.
		  rpm.attachmentBegin(depthImage_.format());		// 0
		  rpm.attachmentSamples(vku::DefaultSampleCount);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eClear);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eUndefined);					// undefined should be used to reset beginning state if load op is clear
		  rpm.attachmentFinalLayout(vk::ImageLayout::eDepthStencilReadOnlyOptimal);			// available for blit/downsampling

		  // A subpass to render using the above attachment
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassDepthStencilAttachment(vk::ImageLayout::eDepthStencilAttachmentOptimal, 0);	// optimal format (read/write) during subpass

		  // **** SUBPASS - Regular rendering //

		  // The depth/stencil attachment.
		  rpm.attachmentBegin(depthImage_.format());		// 1
		  rpm.attachmentSamples(vku::DefaultSampleCount);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eDepthStencilReadOnlyOptimal);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eDepthStencilReadOnlyOptimal);			

		  // The only colour attachment.
		  rpm.attachmentBegin(colorImage_.format());		// 2
		  rpm.attachmentSamples(vku::DefaultSampleCount);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eClear);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eUndefined);					// undefined should be used to reset beginning state if load op is clear
		  rpm.attachmentFinalLayout(vk::ImageLayout::eColorAttachmentOptimal);

		  // A subpass to render using the above two attachments.
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassDepthStencilAttachment(vk::ImageLayout::eDepthStencilReadOnlyOptimal, 1);	// optimal format (read/write) during subpass
		  rpm.subpassColorAttachment(vk::ImageLayout::eColorAttachmentOptimal, 2);
		  
		  // **** SUBPASS - Transparent "Mask" rendering //

		  // The depth/stencil attachment.
		  rpm.attachmentBegin(depthImage_.format());		// 3
		  rpm.attachmentSamples(vku::DefaultSampleCount);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eDepthStencilReadOnlyOptimal);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eDepthStencilReadOnlyOptimal);			

		  // The only colour attachment.
		  rpm.attachmentBegin(colorImage_.format());		// 4
		  rpm.attachmentSamples(vku::DefaultSampleCount);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eColorAttachmentOptimal);		
		  rpm.attachmentFinalLayout(vk::ImageLayout::eColorAttachmentOptimal);

		  // The resolved depth attachment.
		  rpm.attachmentBegin(depthImageResolve_[0].format());		// 5
		  rpm.attachmentSamples(vk::SampleCountFlagBits::e1);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eUndefined);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eShaderReadOnlyOptimal);		// available for sampling after

		  // A subpass to render using the above two attachments.
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassDepthStencilAttachment(vk::ImageLayout::eDepthStencilReadOnlyOptimal, 3);	// optimal format (read/write) during subpass
		  rpm.subpassColorAttachment(vk::ImageLayout::eColorAttachmentOptimal, 4);
		  rpm.subpassResolveDepthStencilAttachment(vk::ImageLayout::eDepthStencilAttachmentOptimal, fw_.getDepthResolveMode(), fw_.getStencilResolveMode(), 5);
		  
		  // A dependency to reset the layout of both attachments.
		  rpm.dependencyBegin(VK_SUBPASS_EXTERNAL, 0);
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eTopOfPipe);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eEarlyFragmentTests| vk::PipelineStageFlagBits::eLateFragmentTests);
		  rpm.dependencySrcAccessMask((vk::AccessFlags)0);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eDepthStencilAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(0, 1);
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eEarlyFragmentTests | vk::PipelineStageFlagBits::eLateFragmentTests);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eEarlyFragmentTests | vk::PipelineStageFlagBits::eLateFragmentTests);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eDepthStencilAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eDepthStencilAttachmentRead);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(VK_SUBPASS_EXTERNAL, 1);
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eTopOfPipe);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencySrcAccessMask((vk::AccessFlags)0);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(1, 2);
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eEarlyFragmentTests | vk::PipelineStageFlagBits::eLateFragmentTests);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eEarlyFragmentTests | vk::PipelineStageFlagBits::eLateFragmentTests);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eDepthStencilAttachmentRead);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eDepthStencilAttachmentRead);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(1, 2);
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(2, VK_SUBPASS_EXTERNAL);	// *resolved* depth available for sampling after final subpass
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eLateFragmentTests);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eFragmentShader);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eDepthStencilAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eShaderRead);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  // Use the maker object to construct the vulkan object
		  renderPass_ = rpm.createUnique(device);
	  }

	  for (int i = 0; i != imageViews_.size(); ++i) {
		  vk::ImageView const attachments[6] = { depthImage_.imageView(), depthImage_.imageView(), colorImage_.imageView(), depthImage_.imageView(), colorImage_.imageView(), depthImageResolve_[0].imageView() };
		  vk::FramebufferCreateInfo const fbci{ {}, *renderPass_, _countof(attachments), attachments, width_, height_, 1 };
		  framebuffers_[eFrameBuffers::COLOR_DEPTH].push_back(device.createFramebufferUnique(fbci));
	  }

	  // Build the renderpass using one attachment, colour   (down - rezzed - resolution pass) **vku::DOWN_RES_FACTOR sets resolution factor of framebuffer
	  {
		  vku::RenderpassMaker rpm;

		  // The colour attachment.
		  rpm.attachmentBegin(colorVolumetricImageDown_.format());
		  rpm.attachmentSamples(vk::SampleCountFlagBits::e1);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eClear);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eUndefined);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eShaderReadOnlyOptimal);	

		  // A subpass to render using the above attachment.
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassColorAttachment(vk::ImageLayout::eColorAttachmentOptimal, 0);


		  // Use the maker object to construct the vulkan object
		  downPass_ = rpm.createUnique(device);
	  }

	  for (int i = 0; i != imageViews_.size(); ++i) {
		  vk::ImageView const attachments[1] = { colorVolumetricImageDown_.imageView() };
		  vk::FramebufferCreateInfo const fbci{ {}, *downPass_, 1, attachments, width_ / vku::DOWN_RES_FACTOR, height_ / vku::DOWN_RES_FACTOR, 1 };
		  framebuffers_[eFrameBuffers::HALF_COLOR_ONLY].push_back(device.createFramebufferUnique(fbci));
	  }

	  // Build the renderpass using one attachment, colour   (mid/intermediatte pass)
	  {
		  vku::RenderpassMaker rpm;

		  // *** 1st SUBPASS - Upsampling and Blend of haLF res volumetric pass
		  // The colour attachment.
		  rpm.attachmentBegin(colorImage_.format());
		  rpm.attachmentSamples(vku::DefaultSampleCount);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eColorAttachmentOptimal);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eColorAttachmentOptimal);

		 // The resolved colour attachment.			// resolve for voxel transparency
		  rpm.attachmentBegin(lastColorImage_.format()); // output color attachment
		  rpm.attachmentSamples(vk::SampleCountFlagBits::e1);
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eUndefined);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eShaderReadOnlyOptimal);

		  // A subpass to render using the above attachment.
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassColorAttachment(vk::ImageLayout::eColorAttachmentOptimal, 0);
		  rpm.subpassResolveAttachment(vk::ImageLayout::eColorAttachmentOptimal, 1);
		  

		  rpm.dependencyBegin(VK_SUBPASS_EXTERNAL, 0); // In
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(0, VK_SUBPASS_EXTERNAL); // Out
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eFragmentShader);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eShaderRead);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  // Use the maker object to construct the vulkan object
		  midPass_ = rpm.createUnique(device);
	  }

	  for (int i = 0; i != imageViews_.size(); ++i) {
												//1st subpass			  //2nd subpass
		  vk::ImageView const attachments[2] = { colorImage_.imageView(), lastColorImage_.imageView() };
		  vk::FramebufferCreateInfo const fbci{ {}, *midPass_, _countof(attachments), attachments, width_, height_, 1 };
		  framebuffers_[eFrameBuffers::MID_COLOR_ONLY].push_back(device.createFramebufferUnique(fbci));
	  }

	  // Build the renderpass (overlay / transparency pass)
	  {
		  vku::RenderpassMaker rpm;

		  // *** 1st SUBPASS - Transparent Voxels
		  // The colour attachment.											// 0
		  rpm.attachmentBegin(colorImage_.format());
		  rpm.attachmentSamples(vku::DefaultSampleCount);
		  // Don't clear the framebuffer for overlay on top of main renderpass
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eColorAttachmentOptimal);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eColorAttachmentOptimal);

		  // The resolved colour attachment.			// resolve for everything else that needs final color buffer w/o Post Postprocessing & GUI
		  rpm.attachmentBegin(lastColorImage_.format()); // output color attachment
		  rpm.attachmentSamples(vk::SampleCountFlagBits::e1);				// 1
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eShaderReadOnlyOptimal);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eShaderReadOnlyOptimal);

		  // The depth/stencil attachment.
		  rpm.attachmentBegin(depthImage_.format());
		  rpm.attachmentSamples(vku::DefaultSampleCount);					// 2
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eDepthStencilReadOnlyOptimal);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eDepthStencilReadOnlyOptimal);			

		  // A subpass to render using the above attachment.
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassColorAttachment(vk::ImageLayout::eColorAttachmentOptimal, 0);
		  rpm.subpassResolveAttachment(vk::ImageLayout::eColorAttachmentOptimal, 1);
		  rpm.subpassDepthStencilAttachment(vk::ImageLayout::eDepthStencilAttachmentOptimal, 2);

		  // *** 2nd SUBPASS - Nuklear GUI
		  // The colour attachment.
		  rpm.attachmentBegin(guiImage_[0].format());
		  rpm.attachmentSamples(vku::DefaultSampleCount);
		  // must clear
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eClear);			// 3
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eUndefined);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eColorAttachmentOptimal);

		  // The resolved colour attachment.			// resolve for GUI
		  rpm.attachmentBegin(guiImage_[1].format());
		  rpm.attachmentSamples(vk::SampleCountFlagBits::e1);				// 4
		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eUndefined);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eColorAttachmentOptimal);

		  // A subpass to render using the above attachment.
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassColorAttachment(vk::ImageLayout::eColorAttachmentOptimal, 3);
		  rpm.subpassResolveAttachment(vk::ImageLayout::eColorAttachmentOptimal, 4);

		  // 2 dependency to reset the layout of attachment.
		  rpm.dependencyBegin(VK_SUBPASS_EXTERNAL, 0); // In
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(0, 1); // subpass -> subpass
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(0, VK_SUBPASS_EXTERNAL); // Out
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eFragmentShader);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eShaderRead);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(1, VK_SUBPASS_EXTERNAL); // Out
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  // Use the maker object to construct the vulkan object
		  overlayPass_ = rpm.createUnique(device);
	  }

	  for (int i = 0; i != imageViews_.size(); ++i) {
											// 1st subpass																  // 2nd subpass                                     
		  vk::ImageView attachments[5] = { colorImage_.imageView(), lastColorImage_.imageView(), depthImage_.imageView(), guiImage_[0].imageView(), guiImage_[1].imageView() };
		  vk::FramebufferCreateInfo fbci{ {}, *overlayPass_, _countof(attachments), attachments, width_, height_, 1 };
		  framebuffers_[eFrameBuffers::COLOR_DEPTHREAD].push_back(device.createFramebufferUnique(fbci));
	  }

	  // Final Pass to Present (Post AA)
	  {
		  vku::RenderpassMaker rpm;

		  // The colour attachment. (multisampled)
		  rpm.attachmentBegin(swapchainImageFormat_);
		  rpm.attachmentSamples(vku::DefaultSampleCount);						// 0

		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eColorAttachmentOptimal);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eColorAttachmentOptimal);

		  // The colour attachment. (resolved)
		  rpm.attachmentBegin(swapchainImageFormat_);
		  rpm.attachmentSamples(vk::SampleCountFlagBits::e1);					 // 1

		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eUndefined);
		  rpm.attachmentFinalLayout(vk::ImageLayout::ePresentSrcKHR);		// any attachment using imageViews_[i] attachments **stay on ePresentSrcKHR explicitly**

		  // A subpass to render using the above attachment.
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassColorAttachment(vk::ImageLayout::eColorAttachmentOptimal, 0);
		  rpm.subpassResolveAttachment(vk::ImageLayout::ePresentSrcKHR, 1);

		  // subpass 1 - overlay final
		  // The colour attachment. (resolved)
		  rpm.attachmentBegin(swapchainImageFormat_);
		  rpm.attachmentSamples(vk::SampleCountFlagBits::e1);					// 2

		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eStore);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::ePresentSrcKHR);
		  rpm.attachmentFinalLayout(vk::ImageLayout::ePresentSrcKHR);

		  // The input attachment. (gui)
		  rpm.attachmentBegin(swapchainImageFormat_);
		  rpm.attachmentSamples(vk::SampleCountFlagBits::e1);					// 3

		  rpm.attachmentLoadOp(vk::AttachmentLoadOp::eLoad);
		  rpm.attachmentStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentStencilLoadOp(vk::AttachmentLoadOp::eDontCare);
		  rpm.attachmentStencilStoreOp(vk::AttachmentStoreOp::eDontCare);
		  rpm.attachmentInitialLayout(vk::ImageLayout::eColorAttachmentOptimal);
		  rpm.attachmentFinalLayout(vk::ImageLayout::eShaderReadOnlyOptimal);

		  // A subpass to render using the above attachment.
		  rpm.subpassBegin(vk::PipelineBindPoint::eGraphics);
		  rpm.subpassColorAttachment(vk::ImageLayout::eColorAttachmentOptimal, 2);
		  rpm.subpassInputAttachment(vk::ImageLayout::eShaderReadOnlyOptimal, vk::ImageAspectFlagBits::eColor, 3);

		  /* Chapter 32 of Vulkan Spec
		  When transitioning the image to VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR or VK_IMAGE_LAYOUT_PRESENT_SRC_KHR, there is no need to delay subsequent processing, or perform any visibility operations (as vkQueuePresentKHR performs automatic visibility operations). To achieve this, the dstAccessMask member of the VkImageMemoryBarrier should be set to 0, and the dstStageMask parameter should be set to VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT.
		  */

		  rpm.dependencyBegin(VK_SUBPASS_EXTERNAL, 0);
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eColorAttachmentRead | vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(VK_SUBPASS_EXTERNAL, 1);
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eFragmentShader);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eInputAttachmentRead);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(0, 1);
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDstAccessMask(vk::AccessFlagBits::eColorAttachmentRead|vk::AccessFlagBits::eColorAttachmentWrite);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  rpm.dependencyBegin(1, VK_SUBPASS_EXTERNAL); // Out To Present
		  rpm.dependencySrcStageMask(vk::PipelineStageFlagBits::eColorAttachmentOutput);
		  rpm.dependencyDstStageMask(vk::PipelineStageFlagBits::eBottomOfPipe);
		  rpm.dependencySrcAccessMask(vk::AccessFlagBits::eColorAttachmentRead);
		  rpm.dependencyDstAccessMask((vk::AccessFlags)0);
		  rpm.dependencyDependencyFlags(vk::DependencyFlagBits::eByRegion);

		  // Use the maker object to construct the vulkan object
		  finalPass_ = rpm.createUnique(device);
	  }

	  for (int i = 0; i != imageViews_.size(); ++i) {
		  vk::ImageView const attachments[4] = { colorImage_.imageView(), imageViews_[i], imageViews_[i], guiImage_[1].imageView() };
		  vk::FramebufferCreateInfo const fbci{ {}, *finalPass_, _countof(attachments), attachments, width_, height_, 1 };
		  framebuffers_[eFrameBuffers::PRESENT].push_back(device.createFramebufferUnique(fbci));
	  }



	  {
		  vk::SemaphoreCreateInfo sci;
		  for (uint32_t i = 0; i < 2; ++i) {
			  semaphores[i].imageAcquireSemaphore_ = device.createSemaphoreUnique(sci);
			  semaphores[i].commandCompleteSemaphore_ = device.createSemaphoreUnique(sci);
			  semaphores[i].transferCompleteSemaphore_[0] = device.createSemaphoreUnique(sci);	// compute transfer
			  semaphores[i].transferCompleteSemaphore_[1] = device.createSemaphoreUnique(sci);	// dynamic transfer
			  semaphores[i].computeCompleteSemaphore_ = device.createSemaphoreUnique(sci); // compute process
		  }
	  }
	  
	  {
		  vk::EventCreateInfo eci;
		  for (int i = 0; i != eGraphicsEvent::_size(); ++i) {
			  graphicsEvents[i] = device.createEventUnique(eci);
		  }
	  }
	  
	  typedef vk::CommandPoolCreateFlagBits ccbits;

	  {
		  vk::CommandPoolCreateInfo cpci{ ccbits::eTransient | ccbits::eResetCommandBuffer, graphicsQueueFamilyIndex };
		  commandPool_[eCommandPools::DEFAULT_POOL] = device.createCommandPoolUnique(cpci);
	  }
	  {
		  vk::CommandPoolCreateInfo cpci{ ccbits::eTransient | ccbits::eResetCommandBuffer, graphicsQueueFamilyIndex };
		  commandPool_[eCommandPools::OVERLAY_POOL] = device.createCommandPoolUnique(cpci);
	  }
	  {
		  vk::CommandPoolCreateInfo cpci{ ccbits::eTransient | ccbits::eResetCommandBuffer, graphicsQueueFamilyIndex };
		  commandPool_[eCommandPools::SCATTERED_POOL] = device.createCommandPoolUnique(cpci);
	  }
	  {

		  vk::CommandPoolCreateInfo cpci{ ccbits::eTransient | ccbits::eResetCommandBuffer, transferQueueFamilyIndex };
		  commandPool_[eCommandPools::DMA_TRANSFER_POOL_THREAD_PRIMARY] = device.createCommandPoolUnique(cpci);
		  commandPool_[eCommandPools::DMA_TRANSFER_POOL_THREAD_SECONDARY] = device.createCommandPoolUnique(cpci);
	  }
	  {
		  vk::CommandPoolCreateInfo cpci{ ccbits::eTransient | ccbits::eResetCommandBuffer, computeQueueFamilyIndex };
		  commandPool_[eCommandPools::COMPUTE_POOL] = device.createCommandPoolUnique(cpci);
	  }

	  // Create draw buffers
	  { // static
		  uint32_t const resource_count((uint32_t)imageViews_.size());
		  vk::CommandBufferAllocateInfo cbai{ *commandPool_[eCommandPools::DEFAULT_POOL], vk::CommandBufferLevel::ePrimary, resource_count };
		  staticDrawBuffers_.allocate(device, cbai);
		 
		  for (uint32_t i = 0; i < resource_count; ++i) {
			  staticCommandsDirty_.emplace_back(false);
			  VKU_SET_OBJECT_NAME(vk::ObjectType::eCommandBuffer, (VkCommandBuffer)*staticDrawBuffers_.cb[0][i], vkNames::CommandBuffer::STATIC);
		  }
	  }
	  {	// present command buffer is fully static and cannot be changed - no diry flag
		  uint32_t const resource_count((uint32_t)imageViews_.size());
		  vk::CommandBufferAllocateInfo cbai{ *commandPool_[eCommandPools::DEFAULT_POOL], vk::CommandBufferLevel::ePrimary, resource_count };
		  presentDrawBuffers_.allocate(device, cbai);
		  for (uint32_t i = 0; i < resource_count; ++i) {
			  VKU_SET_OBJECT_NAME(vk::ObjectType::eCommandBuffer, (VkCommandBuffer)* presentDrawBuffers_.cb[0][i], vkNames::CommandBuffer::PRESENT);
		  }
	  }
	  { // overlay render
		  uint32_t const resource_count((uint32_t)imageViews_.size());
		  vk::CommandBufferAllocateInfo cbai{ *commandPool_[eCommandPools::OVERLAY_POOL], vk::CommandBufferLevel::ePrimary, resource_count };
		  overlayDrawBuffers_.allocate<eOverlayBuffers::RENDER>(device, cbai);
		  for (uint32_t i = 0; i < resource_count; ++i) {
			  VKU_SET_OBJECT_NAME(vk::ObjectType::eCommandBuffer, (VkCommandBuffer)*overlayDrawBuffers_.cb[eOverlayBuffers::RENDER][i], vkNames::CommandBuffer::OVERLAY_RENDER);
		  }
	  }
	  { // scattered
		  vk::CommandBufferAllocateInfo cbai{ *commandPool_[eCommandPools::SCATTERED_POOL], vk::CommandBufferLevel::ePrimary, 1U }; // single resource
		  scatteredDrawBuffer.allocate(device, cbai);
		  scattered_staticCommandsDirty_ = false;
		  VKU_SET_OBJECT_NAME(vk::ObjectType::eCommandBuffer, (VkCommandBuffer)*scatteredDrawBuffer.cb[0][0], vkNames::CommandBuffer::SCATTERED);
	  }

	  {
		  { // dynamic
			  uint32_t const resource_count((uint32_t)imageViews_.size());
			  vk::CommandBufferAllocateInfo cbai{ *commandPool_[eCommandPools::DMA_TRANSFER_POOL_THREAD_PRIMARY], vk::CommandBufferLevel::ePrimary, resource_count };
			  dynamicDrawBuffers_.allocate(device, cbai);
			  for (uint32_t i = 0; i < resource_count; ++i) {
				  VKU_SET_OBJECT_NAME(vk::ObjectType::eCommandBuffer, (VkCommandBuffer)* dynamicDrawBuffers_.cb[0][i], vkNames::CommandBuffer::DYNAMIC);
			  }
		  }
		  { // compute transfer
			  uint32_t const resource_count(2U);
			  vk::CommandBufferAllocateInfo cbai{ *commandPool_[eCommandPools::DMA_TRANSFER_POOL_THREAD_PRIMARY], vk::CommandBufferLevel::ePrimary, resource_count }; // 2 resources
			  computeDrawBuffers_.allocate<eComputeBuffers::TRANSFER_LIGHT>(device, cbai);
			  for (uint32_t i = 0; i < resource_count; ++i) {
				  computeCommandsDirty_[i] = false;
				  VKU_SET_OBJECT_NAME(vk::ObjectType::eCommandBuffer, (VkCommandBuffer)* computeDrawBuffers_.cb[eComputeBuffers::TRANSFER_LIGHT][i], vkNames::CommandBuffer::TRANSFER_LIGHT);
			  }
		  }
		  { // overlay transfer
			  uint32_t const resource_count((uint32_t)imageViews_.size());
			  vk::CommandBufferAllocateInfo cbai{ *commandPool_[eCommandPools::DMA_TRANSFER_POOL_THREAD_SECONDARY], vk::CommandBufferLevel::ePrimary, resource_count };
			  overlayDrawBuffers_.allocate<eOverlayBuffers::TRANSFER>(device, cbai);
			  for (uint32_t i = 0; i < resource_count; ++i) {
				  VKU_SET_OBJECT_NAME(vk::ObjectType::eCommandBuffer, (VkCommandBuffer)*overlayDrawBuffers_.cb[eOverlayBuffers::TRANSFER][i], vkNames::CommandBuffer::OVERLAY_TRANSFER);
			  }
		  }
	  }
	  { // compute render
		  uint32_t const resource_count(2U);
		  vk::CommandBufferAllocateInfo cbai{ *commandPool_[eCommandPools::COMPUTE_POOL], vk::CommandBufferLevel::ePrimary, resource_count };	// 2 resources
		  computeDrawBuffers_.allocate<eComputeBuffers::COMPUTE_LIGHT>(device, cbai);
		  for (uint32_t i = 0; i < resource_count; ++i) {
			  VKU_SET_OBJECT_NAME(vk::ObjectType::eCommandBuffer, (VkCommandBuffer)* computeDrawBuffers_.cb[eComputeBuffers::COMPUTE_LIGHT][i], vkNames::CommandBuffer::COMPUTE_LIGHT);
		  }
	  }

	  preAcquireFirstImage();

    ok_ = true; 
  }

  /// Dump the capabilities of the physical device used by this window.
  void dumpCaps(std::ostream &os, vk::PhysicalDevice pd) const {
    os << "Surface formats\n";
    auto fmts = pd.getSurfaceFormatsKHR(surface_);
    for (auto &fmt : fmts) {
      auto fmtstr = vk::to_string(fmt.format);
      auto cstr = vk::to_string(fmt.colorSpace);
      os << "format=" << fmtstr << " colorSpace=" << cstr << "\n";
    }

    os << "Present Modes\n";
    auto presentModes = pd.getSurfacePresentModesKHR(surface_);
    for (auto pm : presentModes) {
      std::cout << vk::to_string(pm) << "\n";
    }
  }

  static void defaultRenderFunc(vk::CommandBuffer cb, int imageIndex, vk::RenderPassBeginInfo const&rpbi) {
    vk::CommandBufferBeginInfo bi{};
    cb.begin(bi);
    cb.end();
  }

  static_renderpass_function_unconst staticCommandCache;
  scattered_offscreen_renderpass_function_unconst scatteredCommandCache;

  /// Build a static draw buffer. This will be rendered after any dynamic content generated in draw()
  void setStaticCommands(static_renderpass_function static_function, int32_t const iImageIndex = -1) {

	  static vk::ClearValue const clearArray[] = { vk::ClearDepthStencilValue{1.0f, 0}, vk::ClearDepthStencilValue{1.0f, 0}, vk::ClearValue{ std::array<uint32_t, 4>{0, 0, 0, 0}}, vk::ClearDepthStencilValue{1.0f, 0}, vk::ClearValue{ std::array<uint32_t, 4>{0, 0, 0, 0}} };

	  vk::RenderPassBeginInfo rpbi[3];
	  rpbi[eFrameBuffers::COLOR_DEPTH].renderPass = *renderPass_;
	  rpbi[eFrameBuffers::COLOR_DEPTH].renderArea = vk::Rect2D{ {0, 0}, {width_, height_} };
	  rpbi[eFrameBuffers::COLOR_DEPTH].clearValueCount = (uint32_t)_countof(clearArray);
	  rpbi[eFrameBuffers::COLOR_DEPTH].pClearValues = clearArray;

	  rpbi[eFrameBuffers::HALF_COLOR_ONLY].renderPass = *downPass_;
	  rpbi[eFrameBuffers::HALF_COLOR_ONLY].renderArea = vk::Rect2D{ {0, 0}, {width_ / vku::DOWN_RES_FACTOR, height_ / vku::DOWN_RES_FACTOR} };
	  rpbi[eFrameBuffers::HALF_COLOR_ONLY].clearValueCount = 1U;
	  rpbi[eFrameBuffers::HALF_COLOR_ONLY].pClearValues = &clearArray[0];

	  rpbi[eFrameBuffers::MID_COLOR_ONLY].renderPass = *midPass_;
	  rpbi[eFrameBuffers::MID_COLOR_ONLY].renderArea = vk::Rect2D{ {0, 0}, {width_, height_} };
	  rpbi[eFrameBuffers::MID_COLOR_ONLY].clearValueCount = 0U;
	  rpbi[eFrameBuffers::MID_COLOR_ONLY].pClearValues = nullptr;

	  if (iImageIndex < 0) {
		  for (int i = 0; i != staticDrawBuffers_.size(); ++i) {
			  vk::CommandBuffer cb = *staticDrawBuffers_.cb[0][i];
			  rpbi[eFrameBuffers::COLOR_DEPTH].framebuffer = *framebuffers_[eFrameBuffers::COLOR_DEPTH][i];
			  rpbi[eFrameBuffers::HALF_COLOR_ONLY].framebuffer = *framebuffers_[eFrameBuffers::HALF_COLOR_ONLY][i];
			  rpbi[eFrameBuffers::MID_COLOR_ONLY].framebuffer = *framebuffers_[eFrameBuffers::MID_COLOR_ONLY][i];

			  static_renderpass s = { cb, graphicsEvents[eGraphicsEvent::RENDERED_STATIC], rpbi[eFrameBuffers::COLOR_DEPTH], rpbi[eFrameBuffers::HALF_COLOR_ONLY], rpbi[eFrameBuffers::MID_COLOR_ONLY] };
			  static_function(s);

			  staticCommandsDirty_[i] = false;
		  }
		  
		  staticCommandCache = static_function;
	  }
	  else {
		  vk::CommandBuffer cb = *staticDrawBuffers_.cb[0][iImageIndex];
		  rpbi[eFrameBuffers::COLOR_DEPTH].framebuffer = *framebuffers_[eFrameBuffers::COLOR_DEPTH][iImageIndex];
		  rpbi[eFrameBuffers::HALF_COLOR_ONLY].framebuffer = *framebuffers_[eFrameBuffers::HALF_COLOR_ONLY][iImageIndex];
		  rpbi[eFrameBuffers::MID_COLOR_ONLY].framebuffer = *framebuffers_[eFrameBuffers::MID_COLOR_ONLY][iImageIndex];

		  static_renderpass s = { cb, graphicsEvents[eGraphicsEvent::RENDERED_STATIC], rpbi[eFrameBuffers::COLOR_DEPTH], rpbi[eFrameBuffers::HALF_COLOR_ONLY], rpbi[eFrameBuffers::MID_COLOR_ONLY] };
		  static_function(s);

		  staticCommandsDirty_[iImageIndex] = false;
	  }
  }

  void setStaticCommands(scattered_offscreen_renderpass_function scatttered_static_function, bool const cacheCommand = true) {
	vk::CommandBuffer cb = *scatteredDrawBuffer.cb[0][0];
	scattered_offscreen_renderpass s = { cb };
	scatttered_static_function(s);

	scattered_staticCommandsDirty_ = false;

	if (cacheCommand) {
		scatteredCommandCache = scatttered_static_function;
	}
  }
  void setStaticCommandsDirty(static_renderpass_function static_function) {

	  if (static_function == staticCommandCache) {
		  for (int i = 0; i != staticCommandsDirty_.size(); ++i) {
			  staticCommandsDirty_[i] = true;
		  }
	  }
#ifndef NDEBUG
	  assert_print(static_function == staticCommandCache, "[FAIL] No static command cache match");
#endif
  }
  void setStaticCommandsDirty(scattered_offscreen_renderpass_function scatttered_static_function) {
#ifndef NDEBUG
	  assert_print(!scattered_staticCommandsDirty_, "[FAIL] scattered static command aleady dirty");
#endif
	  if (scatttered_static_function == scatteredCommandCache)
		scattered_staticCommandsDirty_ = true;
#ifndef NDEBUG
	  assert_print(scatttered_static_function == scatteredCommandCache, "[FAIL] No scattered static command cache match");
#endif
  }

  /// Build a static draw buffer. This will be rendered after any dynamic content generated in draw()
  void setStaticPresentCommands(present_renderpass_function present_function) {

	  vk::RenderPassBeginInfo rpbi;
	  rpbi.renderPass = *finalPass_;
	  rpbi.renderArea = vk::Rect2D{ {0, 0}, {width_, height_} };
	  rpbi.clearValueCount = 0U;
	  rpbi.pClearValues = nullptr;

		for (uint32_t i = 0; i != presentDrawBuffers_.size(); ++i) {
			vk::CommandBuffer cb = *presentDrawBuffers_.cb[0][i];
			rpbi.framebuffer = *framebuffers_[eFrameBuffers::PRESENT][i];

			present_renderpass s = { cb, rpbi, i };
			present_function(s);
		}
  }
  
//#define VKU_IMPLEMENTATION // debugging enable ONLY to get proper brightness/colors when editing
#ifdef VKU_IMPLEMENTATION

  private:
	  NO_INLINE void fail_acquire_or_present(vk::Result const result, uint32_t& imageIndex, uint32_t& resource_index)
	  {
		  switch (result)
		  {
		  case vk::Result::eSuccess: // should never get here but if we do silently ignore
			  break;
		  case vk::Result::eSuboptimalKHR:	// silently recreate the swap chain, then pre-acquire first image, reset image and resource indices
		  case vk::Result::eErrorOutOfDateKHR:
		  {
			  device_.waitIdle();	// safetly continue after idle detect
			  recreateSwapChain();
			  preAcquireFirstImage();

			  imageIndex = 0;
			  resource_index = 0;
		  }
		  break;
		  default:
			  FMT_LOG_FAIL(GPU_LOG, "Major failure in main render method, < {:s} > \n", vk::to_string(result));
			  break;
		  };
	  }

  public:
  /// Queue the static command buffer for the next image in the swap chain. Optionally call a function to create a dynamic command buffer
  /// for uploading textures, changing uniforms etc.
  void draw(const vk::Device& __restrict device, const vk::Queue& __restrict graphicsQueue, const vk::Queue& __restrict computeQueue, const vk::Queue& __restrict transferQueue,
	  compute_function gpu_compute, dynamic_renderpass_function dynamic_function, overlay_renderpass_function overlay_function, bool const bRenderScatterredPass = false) {

	  // utilize the time between a present() and acquireNextImage()

	  // ######## Acquire *currentframe* //
	  constexpr auto const umax = std::numeric_limits<uint64_t>::max();

	  static uint32_t 
		  resource_index{},   // **** only "compute, dynamic, post_submit_render" should use the resource_index, otherwise use imageIndex ******
		  imageIndex{};		  // dynamic uses imageIndex, but uses resource_index to refer to the objects worked on in post_submit_render

	  // ######### Prepare *currentFrame* //
	  vk::CommandBuffer do_cb[2] = { *dynamicDrawBuffers_.cb[0][imageIndex], *overlayDrawBuffers_.cb[eOverlayBuffers::TRANSFER][imageIndex] };
	  
	  vk::Fence const& dynamic_fence = dynamicDrawBuffers_.fence[0][imageIndex];
	  vk::Fence const& overlay_fence = overlayDrawBuffers_.fence[eOverlayBuffers::TRANSFER][imageIndex];
	  vk::Fence const& dma_transfer_light_fence = computeDrawBuffers_.fence[eComputeBuffers::TRANSFER_LIGHT][resource_index];

	  vk::CommandBuffer compute_upload_light[2] = { *computeDrawBuffers_.cb[eComputeBuffers::TRANSFER_LIGHT][resource_index], nullptr };

	  vk::SubmitInfo submit{};

	  vk::Semaphore const tcSema[2] = { *semaphores[resource_index].transferCompleteSemaphore_[0], *semaphores[resource_index].transferCompleteSemaphore_[1] };
	  
	  { // ######### task group //
		  tbb::task_group tG;

		  { // ######### begin overlay transfer cb update (spawned)
			  overlay_renderpass overlay = { &do_cb[1], nullptr, graphicsEvents }; // avoid lamda heap

			  tG.run(
				  [overlay_function, &overlay, &device, &overlay_fence] {

					  constexpr auto const umax = std::numeric_limits<uint64_t>::max();
					  
					  device.waitForFences(overlay_fence, VK_TRUE, umax);				// protect // overlay fence is overlay Render cb
					  device.resetFences(overlay_fence);

																						// staging
					  overlay_function(overlay);										// submission of staged data to gpu // build transfer cb
				  });
		  }

		  { // ######### begin dynamic transfer cb update (main thread)
			  constexpr auto const umax = std::numeric_limits<uint64_t>::max();

			  vk::Fence fences[2]{ nullptr };
			  uint32_t numFences(0);

			  fences[numFences++] = dynamic_fence;

			  if (computeDrawBuffers_.queued[eComputeBuffers::TRANSFER_LIGHT][resource_index]) {
				  fences[numFences++] = dma_transfer_light_fence;
				  computeDrawBuffers_.queued[eComputeBuffers::TRANSFER_LIGHT][resource_index] = false; // reset
			  }

			  device.waitForFences(numFences, fences, VK_TRUE, umax);		// protect
			  device.resetFences(numFences, fences);

			  resource_control::stage_resources(resource_index);			// staging - must be after protect otherwise odd random flashes of light
			  dynamic_renderpass dynamic = { do_cb[0], resource_index };	
			  dynamic_function(dynamic);									// submission of staged data to gpu

			  if (!computeCommandsDirty_[resource_index])
			  {	
				  compute_gpu_function upload_light = { compute_upload_light[eComputeBuffers::TRANSFER_LIGHT], compute_upload_light[eComputeBuffers::COMPUTE_LIGHT], computeCommandsDirty_[resource_index], resource_index };

				  computeCommandsDirty_[resource_index] = gpu_compute(upload_light);
			  }
		  }

		  // ####### Start Submit *currentframe* //

		  // COMPUTE DMA TRANSFER SUBMIT //
		  if (computeCommandsDirty_[resource_index]) {									// seperated queue sb,it for transfering light and opacity
																						  // so compute only waits on transfer light semaphore, and static waits on opacity semaphore
			  submit.waitSemaphoreCount = 0;												// rather than compute waiting on combined transfer of both light + opacity. Compute only dependent on light.
			  submit.pWaitSemaphores = nullptr;
			  submit.pWaitDstStageMask = nullptr;
			  submit.commandBufferCount = 1;				// submitting dma cb
			  submit.pCommandBuffers = &compute_upload_light[eComputeBuffers::TRANSFER_LIGHT];
			  submit.signalSemaphoreCount = 1;
			  submit.pSignalSemaphores = &tcSema[0];			// signal for compute
			  transferQueue.submit(1, &submit, dma_transfer_light_fence);

			  computeDrawBuffers_.queued[eComputeBuffers::TRANSFER_LIGHT][resource_index] = true;
		  }

		  tG.wait(); // overlay should be done already, no contention

	  } // ######### task group (end) //

	// DYNAMIC & OVERLAY DYNAMIC SUBMIT //
	{
		submit.waitSemaphoreCount = 0;
		submit.pWaitSemaphores = nullptr;			
		submit.pWaitDstStageMask = nullptr;
		submit.commandBufferCount = 2;				// submitting dynamic cb & overlay's dynamic cb
		submit.pCommandBuffers = do_cb;
		submit.signalSemaphoreCount = 1;
		submit.pSignalSemaphores = &tcSema[1];			// signal for dynamic cb in slot 0, signal for overlay dynamic cb in slot 1 (completion)
		transferQueue.submit(1, &submit, dynamic_fence);
	}

	// COMPUTE SUBMIT //
	vk::Semaphore const cSema = { *semaphores[resource_index].computeCompleteSemaphore_ };
	bool bAsyncCompute(false);

	if (computeCommandsDirty_[resource_index]) {
		vk::CommandBuffer compute_process[2] = { nullptr, *computeDrawBuffers_.cb[eComputeBuffers::COMPUTE_LIGHT][resource_index] };
		compute_gpu_function compute_light = { compute_process[eComputeBuffers::TRANSFER_LIGHT], compute_process[eComputeBuffers::COMPUTE_LIGHT], computeCommandsDirty_[resource_index], resource_index };

		vk::Fence const& compute_fence = computeDrawBuffers_.fence[eComputeBuffers::COMPUTE_LIGHT][resource_index];
		if (computeDrawBuffers_.queued[eComputeBuffers::COMPUTE_LIGHT][resource_index]) {
			device.waitForFences(compute_fence, VK_TRUE, umax);
			computeDrawBuffers_.queued[eComputeBuffers::COMPUTE_LIGHT][resource_index] = false; // reset
		}
		device.resetFences(compute_fence);
		computeCommandsDirty_[resource_index] = gpu_compute(compute_light);    // compute part resets the dirty state that transfer set

		vk::PipelineStageFlags waitStages{ vk::PipelineStageFlagBits::eComputeShader };

		submit.waitSemaphoreCount = (uint32_t)computeDrawBuffers_.queued[eComputeBuffers::TRANSFER_LIGHT][resource_index];
		submit.pWaitSemaphores = &tcSema[0];				// waiting on transfer completion only if transfer in progress, otherwise waiting on nothing
		submit.pWaitDstStageMask = &waitStages;
		submit.commandBufferCount = 1;
		submit.pCommandBuffers = &compute_process[eComputeBuffers::COMPUTE_LIGHT];				// submitting compute cb
		submit.signalSemaphoreCount = 1;
		submit.pSignalSemaphores = &cSema;			// signalling compute cb completion
		computeQueue.submit(1, &submit, compute_fence);

		computeDrawBuffers_.queued[eComputeBuffers::COMPUTE_LIGHT][resource_index] = true;
		bAsyncCompute = true;
	}

	//%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%//
	vk::Result result(vk::Result::eSuccess);

		uint32_t realImageIndex(0);
		vk::Semaphore const iaSema = *semaphores[resource_index].imageAcquireSemaphore_;

		result = device.acquireNextImageKHR(*swapchain_, umax, iaSema, vk::Fence(), &realImageIndex);	// **** driver does all of its waiting / spinning here blocking any further execution until ready!!!
																								                    // do any / all updates before this call to spend the time wisely
		[[unlikely]] if (vk::Result::eSuccess != result) {
				fail_acquire_or_present(result, imageIndex, resource_index);
				return;
		}
	//%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%//

	vk::Semaphore const ccSema = *semaphores[resource_index].commandCompleteSemaphore_;
	vk::Semaphore const iatccSema[3] = { iaSema, tcSema[1], cSema };

	// STATIC SUBMIT //
	{
		vk::Fence const& static_fence = staticDrawBuffers_.fence[0][imageIndex];

		device.waitForFences(static_fence, VK_TRUE, umax);
		device.resetFences(static_fence);
		if (staticCommandsDirty_[imageIndex]) {
			setStaticCommands(staticCommandCache, imageIndex);
		}

		vk::CommandBuffer cb = *staticDrawBuffers_.cb[0][imageIndex];
		vk::PipelineStageFlags waitStages[3] = { vk::PipelineStageFlagBits::eVertexInput, vk::PipelineStageFlagBits::eVertexInput, vk::PipelineStageFlagBits::eFragmentShader }; // wait at stage data is required

		submit.waitSemaphoreCount = 2 + (uint32_t)bAsyncCompute;
		submit.pWaitSemaphores = iatccSema;		// waiting on dynamic transfer & input acquire & compute processing
		submit.pWaitDstStageMask = waitStages;
		submit.commandBufferCount = 1;
		submit.pCommandBuffers = &cb;				// submitting static cb
		submit.signalSemaphoreCount = 0;
		submit.pSignalSemaphores = nullptr;			// signalling static cb completion
		
		graphicsQueue.submit(1, &submit, static_fence);
	}

	// SCATTERRED SUBMIT //
	if (bRenderScatterredPass) {	// only on scattered rendering enabled (transient)
		
		vk::Fence cbFenceScattered = scatteredDrawBuffer.fence[0][0];
		device.waitForFences(cbFenceScattered, VK_TRUE, umax);
		device.resetFences(cbFenceScattered);				// have to wait on associatted fence, and reset for next iteration

		if (scattered_staticCommandsDirty_) {
			setStaticCommands(this->scatteredCommandCache, false);
		}
															
		submit.waitSemaphoreCount = 0;						// not waiting on anything, dynamic cb is leveraged for scattered
		submit.pWaitSemaphores = nullptr;					// and the dynamic cb completion was already wait on by previous submit (static cb waiting on dynamic cb completion, signal already cleared)
		submit.pWaitDstStageMask = nullptr;
		submit.commandBufferCount = 1;
		submit.pCommandBuffers = &(*scatteredDrawBuffer.cb[0][0]); // submitting scattered's static cb
		submit.signalSemaphoreCount = 0;
		submit.pSignalSemaphores = nullptr;					// don't care when it completes, scattered's fence provides (parallel operation)
		graphicsQueue.submit(1, &submit, cbFenceScattered); // block here just in case. Scattered rendering only happens at interval of
															// time greater than time to complete the scattered dynamic + static time
															// also singular, not per swapchain image, so at least 1 iteration of all images in swapchain span must be done
	}														// before next invocation of scattered rendering (externally controlled from this function)

	// OVERLAY STATIC SUBMIT //
	vk::CommandBuffer ob = *overlayDrawBuffers_.cb[eOverlayBuffers::RENDER][imageIndex];
	{
		// fence not required ....
		static vk::ClearValue const clearArray[] = { vk::ClearValue{ std::array<uint32_t, 4>{0, 0, 0, 0}}, vk::ClearValue{ std::array<uint32_t, 4>{0, 0, 0, 0}}, vk::ClearValue{ std::array<uint32_t, 4>{0, 0, 0, 0}}, vk::ClearValue{ std::array<uint32_t, 4>{0, 0, 0, 0}} };
		overlay_renderpass overlay = { nullptr, &ob, graphicsEvents, 
			                           vk::RenderPassBeginInfo(*overlayPass_, *framebuffers_[eFrameBuffers::COLOR_DEPTHREAD][imageIndex], vk::Rect2D{ {0, 0}, {width_, height_} }, _countof(clearArray), clearArray) }; // avoid lamda heap
		overlay_function(overlay);  // build render cb

		submit.waitSemaphoreCount = 0;				// waiting on overlay's dynamic cb (slot 0) completion and static cb (slot 1) completion
		submit.pWaitSemaphores = nullptr;			// prior submit already waited on &tcSema[1] (contains semaphor that represents dynamic + overlay transfer)
		submit.pWaitDstStageMask = nullptr;
		submit.commandBufferCount = 1;
		submit.pCommandBuffers = &ob;				// submitting overlay's static cb
		submit.signalSemaphoreCount = 0;
		submit.pSignalSemaphores = nullptr;			// signalling commands complete
		graphicsQueue.submit(1, &submit, overlay_fence);	// ***overlay fence is reset here ok
	}

	// PRESENT (POST AA) FINAL SUBMIT //
	vk::CommandBuffer pb = *presentDrawBuffers_.cb[0][imageIndex];
	{
		vk::Fence cbFencePresent = presentDrawBuffers_.fence[0][imageIndex];
		device.waitForFences(cbFencePresent, VK_TRUE, umax);
		device.resetFences(cbFencePresent);				// have to wait on associatted fence, and reset for next iteration

		submit.waitSemaphoreCount = 0;				
		submit.pWaitSemaphores = nullptr;			
		submit.pWaitDstStageMask = nullptr;
		submit.commandBufferCount = 1;
		submit.pCommandBuffers = &pb;				// submitting presents' static cb
		submit.signalSemaphoreCount = 1;
		submit.pSignalSemaphores = &ccSema;			// signalling commands complete
		graphicsQueue.submit(1, &submit, cbFencePresent);
	}
	
	// ######## Present *currentframe* //
    vk::PresentInfoKHR presentInfo;
    vk::SwapchainKHR swapchain = *swapchain_;
    presentInfo.pSwapchains = &swapchain;
    presentInfo.swapchainCount = 1;
    presentInfo.pImageIndices = &imageIndex;
    presentInfo.waitSemaphoreCount = 1;
    presentInfo.pWaitSemaphores = &ccSema;		// waiting on completion 
    result = presentQueue().presentKHR(presentInfo);		// submit/present to screen queue

	[[unlikely]] if (vk::Result::eSuccess != result) {
		fail_acquire_or_present(result, imageIndex, resource_index);
		return;
	}

	// swapping resources
	resource_index = !resource_index;
	imageIndex = realImageIndex;



  }

#endif

  /// Get the queue used to submit graphics jobs
  const vk::Queue presentQueue() const { return device_.getQueue(presentQueueFamily_, 0); }

  /// Return true if this window was created sucessfully.
  bool ok() const { return ok_; }

  /// Return the renderpass used by this window.
  vk::RenderPass const& __restrict renderPass() const { return(*renderPass_); }
  vk::RenderPass const& __restrict downPass() const { return(*downPass_); }
  vk::RenderPass const& __restrict midPass() const { return(*midPass_); }
  vk::RenderPass const& __restrict overlayPass() const { return(*overlayPass_); }
  vk::RenderPass const& __restrict finalPass() const { return(*finalPass_); }

  /// Return the frame buffers used by this window
  const std::vector<vk::UniqueFramebuffer> &framebuffers(eFrameBuffers const index) const { return framebuffers_[index]; }

  size_t const framebufferCount(eFrameBuffers const index) const { return(framebuffers_[index].size()); }

  /// Destroy resources when shutting down.
  ~Window() {

#if defined(FULLSCREEN_EXCLUSIVE) && defined(VK_EXT_full_screen_exclusive)
	  if (bFullScreenExclusiveSupported) {
		  vkReleaseFullScreenExclusiveModeEXT(device_, *swapchain_);
	  }
#endif

    for (auto &iv : imageViews_) {
      device_.destroyImageView(iv);
    }
	
	computeDrawBuffers_.release(device_);
	staticDrawBuffers_.release(device_);
	dynamicDrawBuffers_.release(device_);
	overlayDrawBuffers_.release(device_);
	scatteredDrawBuffer.release(device_);

    swapchain_ = vk::UniqueSwapchainKHR{};
  }

  Window &operator=(Window &&rhs) = default;

  /// Return the width of the display.
  uint32_t width() const { return width_; }

  /// Return the height of the display.
  uint32_t height() const { return height_; }

  // return image views //
  vk::ImageView const colorImageView() const { return(colorImage_.imageView()); }
  vk::ImageView const guiImageView() const { return(guiImage_[1].imageView()); }
  vk::ImageView const lastColorImageView() const { return(lastColorImage_.imageView()); }
  vk::ImageView const colorvolumetricDownResImageView() const { return(colorVolumetricImageDown_.imageView()); }
  vk::ImageView const depthImageView() const { return(depthImage_.imageView()); }
  vk::ImageView const depthResolvedImageView(uint32_t const index) const { return(depthImageResolve_[index].imageView()); }

  vku::ColorAttachmentImage& colorImage() { return(colorImage_); }
  vku::ColorAttachmentImage& lastColorImage() { return(lastColorImage_); }
  vku::ColorAttachmentImage& colorvolumetricDownResImage() { return(colorVolumetricImageDown_); }

  /// Return the format of the back buffer.
  vk::Format depthImageFormat() const { return depthImage_.format(); }

  /// Return the format of the back buffer.
  vk::Format swapchainImageFormat() const { return swapchainImageFormat_; }

  /// Return the colour space of the back buffer (Usually sRGB)
  vk::ColorSpaceKHR swapchainColorSpace() const { return swapchainColorSpace_; }

  /// Return the swapchain object
  const vk::SwapchainKHR swapchain() const { return *swapchain_; }

  /// Return the views of the swap chain images
  const std::vector<vk::ImageView> &imageViews() const { return imageViews_; }

  /// Return the swap chain images
  const std::vector<vk::Image> &images() const { return images_; }

  vk::Fence const& scatteredRenderingFence() const { return(scatteredDrawBuffer.fence[0][0]); }

  /// Return a defult command Pool to use to create new command buffers.
  vk::CommandPool const& commandPool(eCommandPools const index) const { return(*commandPool_[index]); }

  /// Return the number of swap chain images.
  int numImageIndices() const { return (int)images_.size(); }

  bool const isFullScreenExclusiveSupported() const { return(bFullScreenExclusiveSupported); }


private:
  vk::Instance instance_;
  vk::SurfaceKHR surface_;
  vk::UniqueSwapchainKHR swapchain_;
  vk::UniqueRenderPass renderPass_, downPass_, midPass_, overlayPass_, finalPass_;
  
  struct semaphores {
	  vk::UniqueSemaphore imageAcquireSemaphore_;
	  vk::UniqueSemaphore commandCompleteSemaphore_;
	  vk::UniqueSemaphore transferCompleteSemaphore_[2];
	  vk::UniqueSemaphore computeCompleteSemaphore_;
  } semaphores[2];
 
  vk::UniqueEvent	  graphicsEvents[eGraphicsEvent::_size()];
  
  vk::UniqueCommandPool		commandPool_[eCommandPools::_size()];

  std::vector<vk::ImageView> imageViews_;
  std::vector<vk::Image> images_;

  std::vector<vk::UniqueFramebuffer> framebuffers_[eFrameBuffers::_size()];
  CommandBufferContainer<eComputeBuffers::_size()> computeDrawBuffers_;	// one for transfer, one for computing ....
  CommandBufferContainer<1> staticDrawBuffers_;
  CommandBufferContainer<1> dynamicDrawBuffers_;
  CommandBufferContainer<eOverlayBuffers::_size()> overlayDrawBuffers_;	// one for transfer, one for rendering
  CommandBufferContainer<1> scatteredDrawBuffer;
  CommandBufferContainer<1> presentDrawBuffers_;

  vku::ColorAttachmentImage colorImage_, guiImage_[2];
  vku::ColorAttachmentImage lastColorImage_;  // not antialiased and does not contain GUI, for that use PostAA lastColorImage - cPostProcess.h
  vku::ColorAttachmentImage colorVolumetricImageDown_;
  vku::DepthAttachmentImage depthImage_;
  vku::DepthImage			depthImageResolve_[2];

  uint32_t presentQueueFamily_ = 0, graphicsQueueFamily_ = 0;
  uint32_t width_;
  uint32_t height_;
  vk::Format swapchainImageFormat_ = vk::Format::eB8G8R8A8Unorm;
  vk::ColorSpaceKHR swapchainColorSpace_ = vk::ColorSpaceKHR::eSrgbNonlinear;
  vk::Device device_;
  // neccessary for swapchain hot recreation:
  vk::PhysicalDevice physicalDevice_;
  HMONITOR monitor_ = nullptr;
  bool vsyncDisabled_ = false;

  bool ok_ = false;

  std::vector<bool> staticCommandsDirty_;
  bool computeCommandsDirty_[2] = { false };
  bool scattered_staticCommandsDirty_ = false;

  // extensions enabled ? //
  bool bFullScreenExclusiveSupported = false;
};

} // namespace vku

#endif // VKU_FRAMEWORK_HPP
